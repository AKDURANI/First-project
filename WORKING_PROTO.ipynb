{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "p8qrL9rVZE8r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install transformers\\n!pip install pydub\\n!pip install -U openai-whisper\\n!pip install ffmpeg-python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip install transformers\n",
    "!pip install pydub\n",
    "!pip install -U openai-whisper\n",
    "!pip install ffmpeg-python\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28Tefn8fZ3Px",
    "outputId": "c729aa64-7f9f-4863-c139-105b56ed931a"
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-W6oxfHNaDY5",
    "outputId": "3de0fc0a-38ea-4c05-894d-391cecff0ff5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install setuptools-rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#conda install -c conda-forge google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install Js2Py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wavio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sounddevice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "X6JEAD4vLeOU"
   },
   "outputs": [],
   "source": [
    "# audio = get_audio()\n",
    "# audio_array = audio_segment_to_ndarray(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch accelerate torchaudio datasets\n",
    "#!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/huggingface/transformers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
    "import torch\n",
    "\n",
    "model_id = \"facebook/mms-1b-all\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model7 = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "#model7 = model7.to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtVOfYd5e_nk"
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "model0 = whisper.load_model(\"small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwvhptHlg5Sa",
    "outputId": "bcfdf7a7-738b-4d55-f6ff-b1056b9f889c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install transformers --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip show transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\ANACONDA\\\\python.exe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!\"C:\\\\ANACONDA\\\\python.exe\" -m pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "v-jURlXF4M-w"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "dialouge = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q torchaudio omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "complicated-receiver"
   },
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from pprint import pprint\n",
    "from omegaconf import OmegaConf\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "torch.hub.download_url_to_file('https://raw.githubusercontent.com/snakers4/silero-models/master/models.yml',\n",
    "                               'latest_silero_models.yml',\n",
    "                               progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14f49758",
    "outputId": "9d1872c7-67fb-4b24-b5b6-025766503dc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Muhammad Ali Khan/.cache\\torch\\hub\\snakers4_silero-models_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "language = 'en'\n",
    "model_id = 'v3_en'\n",
    "device = torch.device('cuda')\n",
    "\n",
    "modelstr, example_text = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                     model='silero_tts',\n",
    "                                     language=language,\n",
    "                                     speaker=model_id)\n",
    "#modelstr = torch.modelstr('cuda')  # gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Ali Khan\\Desktop\\LALA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def calculate_execution_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' execution time: {execution_time:.6f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "from scipy.io import wavfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [02/Aug/2023 11:07:57] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:07:57] \"GET /staticFiles/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:07:57] \"GET /staticFiles/index2.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:07:57] \"GET /staticFiles/index2.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Recieved!\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "[2023-08-02 11:08:02,680] ERROR in app: Exception on /predict [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Muhammad Ali Khan\\AppData\\Local\\Temp\\ipykernel_6440\\1990693780.py\", line 97, in predict\n",
      "    predictions = ml_model_prediction(audio_blob)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Muhammad Ali Khan\\AppData\\Local\\Temp\\ipykernel_6440\\1990693780.py\", line 74, in ml_model_prediction\n",
      "    audio = modelstr.apply_tts(text=example_text,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<torch_package_0>.multi_acc_v3_package.py\", line 316, in apply_tts\n",
      "    sentences, clean_sentences, break_lens, prosody_rates, prosody_pitches, sp_ids = self.prepare_tts_model_input(input_text,\n",
      "                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<torch_package_0>.multi_acc_v3_package.py\", line 63, in prepare_tts_model_input\n",
      "    clean_text_list = self.process_simple_text(text)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<torch_package_0>.multi_acc_v3_package.py\", line 91, in process_simple_text\n",
      "    raise ValueError\n",
      "ValueError\n",
      "127.0.0.1 - - [02/Aug/2023 11:08:02] \"POST /predict HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Transcription:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Aug/2023 11:09:57] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:09:57] \"GET /staticFiles/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:09:57] \"GET /staticFiles/index2.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:09:57] \"GET /staticFiles/index2.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Recieved!\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "[2023-08-02 11:10:00,868] ERROR in app: Exception on /predict [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Muhammad Ali Khan\\AppData\\Local\\Temp\\ipykernel_6440\\1990693780.py\", line 97, in predict\n",
      "    predictions = ml_model_prediction(audio_blob)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Muhammad Ali Khan\\AppData\\Local\\Temp\\ipykernel_6440\\1990693780.py\", line 74, in ml_model_prediction\n",
      "    audio = modelstr.apply_tts(text=example_text,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<torch_package_0>.multi_acc_v3_package.py\", line 316, in apply_tts\n",
      "    sentences, clean_sentences, break_lens, prosody_rates, prosody_pitches, sp_ids = self.prepare_tts_model_input(input_text,\n",
      "                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<torch_package_0>.multi_acc_v3_package.py\", line 63, in prepare_tts_model_input\n",
      "    clean_text_list = self.process_simple_text(text)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<torch_package_0>.multi_acc_v3_package.py\", line 91, in process_simple_text\n",
      "    raise ValueError\n",
      "ValueError\n",
      "127.0.0.1 - - [02/Aug/2023 11:10:00] \"POST /predict HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Transcription:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Aug/2023 11:10:57] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:10:57] \"GET /staticFiles/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:10:57] \"GET /staticFiles/index2.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:10:57] \"GET /staticFiles/index2.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Recieved!\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "[2023-08-02 11:11:02,291] ERROR in app: Exception on /predict [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ANACONDA\\Lib\\site-packages\\flask\\app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Muhammad Ali Khan\\AppData\\Local\\Temp\\ipykernel_6440\\1990693780.py\", line 97, in predict\n",
      "    predictions = ml_model_prediction(audio_blob)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Muhammad Ali Khan\\AppData\\Local\\Temp\\ipykernel_6440\\1990693780.py\", line 74, in ml_model_prediction\n",
      "    audio = modelstr.apply_tts(text=example_text,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<torch_package_0>.multi_acc_v3_package.py\", line 316, in apply_tts\n",
      "    sentences, clean_sentences, break_lens, prosody_rates, prosody_pitches, sp_ids = self.prepare_tts_model_input(input_text,\n",
      "                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<torch_package_0>.multi_acc_v3_package.py\", line 63, in prepare_tts_model_input\n",
      "    clean_text_list = self.process_simple_text(text)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<torch_package_0>.multi_acc_v3_package.py\", line 91, in process_simple_text\n",
      "    raise ValueError\n",
      "ValueError\n",
      "127.0.0.1 - - [02/Aug/2023 11:11:02] \"POST /predict HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Transcription:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Aug/2023 11:11:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:11:05] \"GET /staticFiles/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:11:05] \"GET /staticFiles/index2.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [02/Aug/2023 11:11:05] \"GET /staticFiles/index2.js HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template, send_file\n",
    "import os\n",
    "import soundfile as sf\n",
    "import io\n",
    "import base64\n",
    "import scipy.signal as signal\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "app = Flask(__name__, template_folder='templateFiles', static_folder='staticFiles')\n",
    "\n",
    "# Replace this function with your ML model prediction logic\n",
    "def ml_model_prediction(audio_blob):\n",
    "    # Simulate ML model prediction - replace this with actual ML model interaction\n",
    "    \n",
    "    #convert_blob_to_wav(audio_blob, \"output.wav\")\n",
    "    audio_blob.seek(0)\n",
    "    \n",
    "    #audio_np = np.array(audio_data , dtype=np.float32\n",
    "    audio_data, _ = sf.read(io.BytesIO(audio_blob.read()))\n",
    "    sf.write(\"outputx.wav\", audio_data,48000 )\n",
    "    \n",
    "    #...................................................................\n",
    "    \n",
    "    # Read the audio file\n",
    "    sample_rate, audio_data = wav.read(\"outputx.wav\")\n",
    "\n",
    "    # Calculate the resampling ratio\n",
    "    resampling_ratio = float(16000) / 48000\n",
    "\n",
    "    # Perform the resampling\n",
    "    import scipy.signal as signal\n",
    "    resampled_audio_data = signal.resample(audio_data, int(len(audio_data) * resampling_ratio))\n",
    "    print(type(resampled_audio_data) )\n",
    "    # Save the resampled audio to the output file\n",
    "    wav.write(\"output.wav\", 16000, resampled_audio_data.astype(audio_data.dtype))\n",
    "    sample_rate = wav.read(\"output.wav\")\n",
    "    \n",
    "    #...................................................................\n",
    "    \n",
    "    # Read the WAV file using scipy.io.wavfile\n",
    "    sample_rate, audio_data = wavfile.read(\"output.wav\")\n",
    "\n",
    "    # Convert the audio data to a NumPy array\n",
    "    numpy_array = np.array(audio_data, dtype=np.float32)\n",
    "    \n",
    "    #...................................................................\n",
    "    \n",
    "    inputs = processor(numpy_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model7(**inputs).logits\n",
    "\n",
    "    ids = torch.argmax(outputs, dim=-1)[0]\n",
    "    transcription = processor.decode(ids)\n",
    "    print(type(transcription))\n",
    "    print(\"Transcription: \",transcription)\n",
    "    \n",
    "    #...................................................................\n",
    "    \n",
    "    for step in range(1):\n",
    "        new_user_input_ids = tokenizer.encode(transcription + tokenizer.eos_token, return_tensors='pt')\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "        chat_history_ids = dialouge.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    f_text = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    #print(f_text)\n",
    "  #.................................................\n",
    "    sample_rate = 48000\n",
    "    speaker = 'en_1'\n",
    "    put_accent=True\n",
    "    put_yo=True\n",
    "    example_text = f_text\n",
    "    audio = modelstr.apply_tts(text=example_text,\n",
    "                          speaker=speaker,\n",
    "                          sample_rate=sample_rate,\n",
    "                          put_accent=put_accent,\n",
    "                          put_yo=put_yo)\n",
    "\n",
    "    return audio\n",
    "    \n",
    "# Serve the index.html file\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('lala -2.html')\n",
    "\n",
    "# API endpoint for audio file upload and ML model prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    print(\"Audio Recieved!\")\n",
    "    audio_blob = request.files['audio']\n",
    "\n",
    "    if not audio_blob:\n",
    "        return jsonify({'error': 'No audio file provided.'}), 400\n",
    "\n",
    "    # Perform prediction using ML model\n",
    "    predictions = ml_model_prediction(audio_blob)\n",
    "    \"\"\"audio_bytes = predictions.to('cpu').detach().numpy().tobytes()\n",
    "    audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')\n",
    "    response = {\n",
    "        'audio': audio_base64\n",
    "    }\n",
    "     return jsonify(response),200\"\"\"\n",
    "    sf.write(\"output2.wav\", predictions,48000 )\n",
    "    \n",
    "    audio_path = 'C:/Users/Muhammad Ali Khan/Desktop/LALA/output2.wav'\n",
    "    return send_file(audio_path, mimetype='output2/wav')\n",
    "   \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
